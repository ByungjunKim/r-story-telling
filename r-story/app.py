import gradio as gr
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# ë²¡í„° DB
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
vectorstore = FAISS.load_local("solo_leveling_faiss_ko", embeddings, allow_dangerous_deserialization=True)

# ëª¨ë¸ ë¡œë”©
model_name = "kakaocorp/kanana-nano-2.1b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to("cuda")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=200)
lm = pipe

# ì„ íƒì§€
choices = [
    "1-1. í™©ë™ì„ ë¬´ë¦¬ë¥¼ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.",
    "1-2. í™©ë™ì„ ë¬´ë¦¬ì™€ ì§„í˜¸ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.",
    "2. í™©ë™ì„ ë¬´ë¦¬ë¥¼ ê¸°ì ˆì‹œí‚¤ê³  ì‚´ë ¤ë‘”ë‹¤.",
    "3-1. ë§ˆì •ì„ì„ ë“¤ê³  ë„ë§ì¹œë‹¤.",
    "3-2. ì‹œìŠ¤í…œì„ ê±°ë¶€í•˜ê³  ê·¸ëƒ¥ ë„ë§ì¹œë‹¤."
]

# RAG + ëŒ€ì‚¬ ìƒì„± í•¨ìˆ˜
def rag_answer(message, history):
    try:
        user_idx = int(message.strip()) - 1
        user_choice = choices[user_idx]
    except:
        return "â—ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 1 ~ 5)"

    docs = vectorstore.similarity_search(user_choice, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""
ë‹¹ì‹ ì€ ì›¹íˆ° 'ë‚˜ í˜¼ìë§Œ ë ˆë²¨ì—…'ì˜ ì„±ì§„ìš°ì…ë‹ˆë‹¤.

í˜„ì¬ ìƒí™©:
{context}

ì‚¬ìš©ì ì„ íƒ: {user_choice}

ì„±ì§„ìš°ì˜ ë§íˆ¬ë¡œ ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ì‚¬ë¥¼ 1~2ë¬¸ì¥ ìƒì„±í•˜ì„¸ìš”.
ì¤‘ë³µëœ ë‚´ìš©ì´ë‚˜ ë¹„ìŠ·í•œ ë¬¸ì¥ì€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
"""
    response = lm(prompt)[0]["generated_text"]
    return response

# Gradio ì¸í„°í˜ì´ìŠ¤
demo = gr.ChatInterface(
    fn=rag_answer, 
    title="ğŸ”¥ ì„±ì§„ìš° ì„ íƒí˜• RAG ëŒ€ì‚¬ ìƒì„±ê¸°",
    description="1~5 ì¤‘ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ì„ íƒì— ë”°ë¥¸ ì„±ì§„ìš°ì˜ ëŒ€ì‚¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
)
demo.launch(share=True)
