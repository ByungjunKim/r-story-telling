# rag_gradio.py

import gradio as gr
import torch
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline

# ğŸ”¹ 1. ë²¡í„°DB ë¡œë“œ
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
vectorstore = FAISS.load_local("solo_leveling_faiss_ko", embeddings, allow_dangerous_deserialization=True)

# ğŸ”¹ 2. Kanana ëª¨ë¸ ë¡œë“œ
model_name = "kakaocorp/kanana-nano-2.1b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to("cuda")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
llm = HuggingFacePipeline(pipeline=pipe)

# ğŸ”¹ 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
ë‹¤ìŒì€ ë°°ê²½ ì§€ì‹ì…ë‹ˆë‹¤:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""
prompt = PromptTemplate(input_variables=["context", "question"], template=template)

def rag_answer(message, history):
    docs = vectorstore.similarity_search(message, k=3)
    context = "\n".join([doc.page_content for doc in docs])
    final_prompt = prompt.format(context=context, question=message)
    response = llm(final_prompt)
    return response


# ğŸ”¹ 5. Gradio ì¸í„°í˜ì´ìŠ¤ ì •ì˜
demo = gr.ChatInterface(fn=rag_answer, title="ğŸ§  Kanana ê¸°ë°˜ RAG ì§ˆì˜ì‘ë‹µ")
demo.launch(share=True)  # ğŸ”— ì™¸ë¶€ ì ‘ì† ì£¼ì†Œ ìë™ ìƒì„±
