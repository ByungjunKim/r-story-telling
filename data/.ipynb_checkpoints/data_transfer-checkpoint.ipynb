{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb48a0d3-aabc-4649-a4b8-7d16770b72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"Solo_Leveling_webtoon - full_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ í†µì¼\n",
    "df = df.rename(columns={\n",
    "    'ëŒ€ì‚¬':'dialogue',\n",
    "    'ë‚´ì ì„¤ëª…':'monologue',\n",
    "    'ì‹œìŠ¤í…œ':'system'\n",
    "})\n",
    "\n",
    "# meltë¡œ ì„¸ë¡œë¡œ ë³€í™˜\n",
    "long_df = df.melt(\n",
    "    id_vars=['ì—í”¼ì†Œë“œëª…'],  # í™”ìˆ˜ ë˜ëŠ” episode ì»¬ëŸ¼\n",
    "    value_vars=['dialogue', 'monologue', 'system'],\n",
    "    var_name='type',\n",
    "    value_name='content'\n",
    ")\n",
    "\n",
    "# NaN ì œê±°\n",
    "long_df = long_df.dropna(subset=['content']).reset_index(drop=True)\n",
    "\n",
    "# type í•œêµ­ì–´í™”\n",
    "long_df['type'] = long_df['type'].map({\n",
    "    'dialogue':'ëŒ€ì‚¬', \n",
    "    'monologue':'ë‚´ì ì„¤ëª…',\n",
    "    'system':'ì‹œìŠ¤í…œ'\n",
    "})\n",
    "\n",
    "# ì €ì¥\n",
    "long_df.to_csv(\"full_data_cleaned.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c559a12-6e85-42fe-ad54-ffcfedf63904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sl_dialogue.tsv      sl_skill.tsv                             untitled.txt\n",
      " sl_personality.tsv  'Solo_Leveling_webtoon - full_data.tsv'\n",
      " sl_situation.tsv     Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09c0d9c-77b1-4640-9ce7-43d97b16f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³€í™˜ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: Solo_Leveling_webtoon - sequential.tsv\n",
      "         ì—í”¼ì†Œë“œ                     scene_text  type\n",
      "0  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „    ë„¤, ê¹€ìƒì‹ ì•„ì €ì”¨. ì‹ ê²½ ì¨ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.    ëŒ€ì‚¬\n",
      "1  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „               ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°  ë‚´ì ì„¤ëª…\n",
      "2  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „         ë­˜ìš” í•˜í•˜... ì˜¤ëŠ˜ë„ ì˜ ë¶€íƒë“œë¦´ê²Œìš”.    ëŒ€ì‚¬\n",
      "3  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „    í—Œí„°í˜‘íšŒ ì†Œì† ì¤‘ ê°€ì¥ ë‚®ì€ ê³„ê¸‰, ìµœì•½ì˜ í—Œí„°.  ë‚´ì ì„¤ëª…\n",
      "4  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „  ì–´? ì•ˆë…•í•˜ì„¸ìš”. ì£¼í¬ ì”¨ë„ ì´ë²ˆ ë ˆì´ë“œ ê°€ì‹œëŠ”êµ°ìš”.    ëŒ€ì‚¬\n",
      "5  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „   ë‚˜ì—ê²Œ ì´ëŸ° ì¼ì´ ì¼ì–´ë‚  ì¤„ì€...ìƒìƒë„ ëª» í–ˆë‹¤.  ë‚´ì ì„¤ëª…\n",
      "6  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „            ì–´ì©Œë‹¤ë³´ë‹ˆ ê·¸ë ‡ê²Œ ëë„¤ìš” í•˜í•˜...    ëŒ€ì‚¬\n",
      "7  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „                        ëŒ€í•œë¯¼êµ­ ì„œìš¸  ë‚´ì ì„¤ëª…\n",
      "8  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „         Eê¸‰ ë˜ì „ì´ì—ˆëŠ”ë° ì €ë§Œ ë‹¤ì³ì„œ ë‚˜ì™”ì–´ìš”.    ëŒ€ì‚¬\n",
      "9  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „                     Eê¸‰ í—Œí„° ì„±ì§„ìš°.  ë‚´ì ì„¤ëª…\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "input_path = \"Solo_Leveling_webtoon - full_data.tsv\"\n",
    "output_path = \"Solo_Leveling_webtoon - sequential.tsv\"\n",
    "\n",
    "# TSV ì½ê¸°\n",
    "df = pd.read_csv(input_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜(NaN)ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# 2. ì—´ ì´ë¦„ í™•ì¸ (ì—°êµ¬ì›ë‹˜ íŒŒì¼ì— ë§ì¶° ì¡°ì •)\n",
    "cols = ['ì—í”¼ì†Œë“œëª…', 'ëŒ€ì‚¬', 'ë‚´ì ì„¤ëª…', 'ì‹œìŠ¤í…œ']\n",
    "\n",
    "# 3. Sequential Interleaving ë³€í™˜\n",
    "rows = []\n",
    "for ep, group in df.groupby('ì—í”¼ì†Œë“œëª…'):\n",
    "    for idx, row in group.iterrows():\n",
    "        # ëŒ€ì‚¬, ë‚´ì ì„¤ëª…, ì‹œìŠ¤í…œ ìˆœì„œëŒ€ë¡œ ì´ì–´ ë¶™ì„\n",
    "        for col in ['ëŒ€ì‚¬', 'ë‚´ì ì„¤ëª…', 'ì‹œìŠ¤í…œ']:\n",
    "            text = row[col].strip()\n",
    "            if text:  # ë¹ˆ ì¹¸ì€ ê±´ë„ˆë›´ë‹¤\n",
    "                rows.append({\n",
    "                    'ì—í”¼ì†Œë“œ': ep,\n",
    "                    'scene_text': text,\n",
    "                    'type': col  \n",
    "                })\n",
    "\n",
    "seq_df = pd.DataFrame(rows)\n",
    "\n",
    "# 4. ì €ì¥\n",
    "seq_df.to_csv(output_path, sep='\\t', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"ë³€í™˜ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜:\", output_path)\n",
    "print(seq_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ff8f80-44df-4dc3-8c71-1095290ba9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¹ì‹ ì€ ì„±ì§„ìš°ì…ë‹ˆë‹¤.\n",
      "ë‹¤ìŒì€ ê³¼ê±°ì˜ í–‰ë™ê³¼ ëŒ€ì‚¬ ê¸°ë¡ì…ë‹ˆë‹¤:\n",
      "ìƒê° ì´ìƒìœ¼ë¡œ ì²´ë ¥ì´ ê¹ì—¬ ìˆì—ˆë‚˜?\n",
      "í˜¹ì‹œ... ëŠ¥ë ¥ì¹˜ê°€ ì˜¬ë¼ê°ˆìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ë¶™ëŠ” ê±¸ê¹Œ?\n",
      "ê°•í•´ì§€ë ¤ê³  ì—¬ê¸°ê¹Œì§€ ì™”ì–ì•„.\n",
      "\n",
      "í˜„ì¬ ìƒí™©: í™©ë™ì„ ë¬´ë¦¬ë¥¼ ë§Œë‚¬ì„ ë•Œ ì–´ë–»ê²Œ í–‰ë™í• ê¹Œ?\n",
      "ì„±ì§„ìš°ë‹µê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
      "í™©ë™ì„ ë¬´ë¦¬ë¥¼ ë¬´ë ¥í™”ì‹œí‚¬ ê³„íšì„ ì„¸ìš¸ ê²ƒì…ë‹ˆë‹¤.   ë‹µì€ ì •í•´ì ¸ ìˆì§€ ì•Šì§€ë§Œ, ì„±ì§„ìš°ì˜ ì„±ê²©ìƒ ê°•ë ¥í•œ ì „ëµì„ êµ¬ì‚¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.   #ì„±ì§„ìš° #í™©ë™ì„ë¬´ë¦¬ #ì „ëµ #ê°•ë ¥í•œê³„íš #ì„±ê²© #ë¬´ë ¥í™” #ì•ìœ¼ë¡œì˜í–‰ë™ #ì˜ì§€ #ê³ ë“±í•™ìƒ #ìºë¦­í„° #ì—­í•  #ì´ì•¼ê¸° #ê²Œì„\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"sl_webtoon_full_data_sequential.tsv\", sep=\"\\t\")\n",
    "texts = df['scene_text'].fillna(\"\").tolist()\n",
    "\n",
    "# 2ï¸âƒ£ ë¬¸ì¥ ì„ë² ë”© & ë²¡í„° DB êµ¬ì¶•\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # CPU/GPU ëª¨ë‘ ê°€ëŠ¥\n",
    "embeddings = model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# 3ï¸âƒ£ ì¹´ë‚˜ë‚˜ ëª¨ë¸ ë¡œë“œ\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"kakaocorp/kanana-nano-2.1b-instruct\",\n",
    "    device=0  # GPU ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# 4ï¸âƒ£ ì§ˆì˜ â†’ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰\n",
    "def retrieve_context(query, top_k=3):\n",
    "    q_emb = model.encode([query], convert_to_tensor=False)\n",
    "    distances, indices = index.search(q_emb, top_k)\n",
    "    return [texts[i] for i in indices[0]]\n",
    "\n",
    "# 5ï¸âƒ£ ì¹´ë‚˜ë‚˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‘ë‹µ\n",
    "def ask_kanana(query):\n",
    "    context = \"\\n\".join(retrieve_context(query))\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ ì„±ì§„ìš°ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒì€ ê³¼ê±°ì˜ í–‰ë™ê³¼ ëŒ€ì‚¬ ê¸°ë¡ì…ë‹ˆë‹¤:\n",
    "{context}\n",
    "\n",
    "í˜„ì¬ ìƒí™©: {query}\n",
    "ì„±ì§„ìš°ë‹µê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "    result = generator(prompt, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# ğŸ”¹ í…ŒìŠ¤íŠ¸\n",
    "print(ask_kanana(\"í™©ë™ì„ ë¬´ë¦¬ë¥¼ ë§Œë‚¬ì„ ë•Œ ì–´ë–»ê²Œ í–‰ë™í• ê¹Œ?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae1a74d-b669-482e-95c7-9ad3d7ecb52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG DB êµ¬ì¶• ì™„ë£Œ, ì´ ë¬¸ì¥ ìˆ˜: 549\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data_path = \"sl_webtoon_full_data_sequential.tsv\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "\n",
    "df[\"text\"] = df[[\"ì—í”¼ì†Œë“œ\", \"scene_text\", \"type\"]].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = model.encode(df[\"text\"].tolist(), convert_to_numpy=True)\n",
    "\n",
    "#FAISS ì¸ë±ìŠ¤ êµ¬ì¶•\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  \n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"solo_leveling_faiss.index\")\n",
    "with open(\"solo_leveling_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df[\"text\"].tolist(), f)\n",
    "\n",
    "print(\"RAG DB êµ¬ì¶• ì™„ë£Œ, ì´ ë¬¸ì¥ ìˆ˜:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a30fca-d526-4902-91a6-d1ec4c8004b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìƒˆ RAG DB êµ¬ì¶• ì™„ë£Œ, ë¬¸ì¥ ìˆ˜: 549\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "data_path = \"sl_webtoon_full_data_sequential.tsv\"\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "df[\"text\"] = df[[\"ì—í”¼ì†Œë“œ\", \"scene_text\", \"type\"]].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "# 2. í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸\n",
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "\n",
    "# 3. ì„ë² ë”© ìƒì„±\n",
    "embeddings = model.encode(df[\"text\"].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# 4. ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•\n",
    "dim = embeddings.shape[1]  # ëª¨ë¸ ì¶œë ¥ ì°¨ì› ìë™ í™•ì¸\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "# 5. ì €ì¥\n",
    "faiss.write_index(index, \"solo_leveling_faiss_ko.index\")\n",
    "with open(\"solo_leveling_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df[\"text\"].tolist(), f)\n",
    "\n",
    "print(\"âœ… ìƒˆ RAG DB êµ¬ì¶• ì™„ë£Œ, ë¬¸ì¥ ìˆ˜:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f841c6-ba70-45ee-970e-98199090275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ìœ ì‚¬ ì¥ë©´ Top 10 ğŸ”¹\n",
      "\n",
      "[ì ìˆ˜: 83.3571] 1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „ ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„° ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 88.2755] 2ê¶Œ_4í™”_ë³´ìŠ¤ì „ í—Œí„° ì„±ì§„ìš°ì…ë‹ˆë‹¤. ëŒ€ì‚¬\n",
      "[ì ìˆ˜: 93.2208] 1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „ Eê¸‰ í—Œí„° ì„±ì§„ìš°. ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 99.7465] 3ê¶Œ_7í™”_ì´ìƒí•œë ˆì´ë“œ ê·¸ ë…€ì„ì´ ì›í•˜ëŠ” ê±´ ì‹¤ë ¥ì€ ìˆì§€ë§Œ ë“±ê¸‰ì´ ë‚®ì€ í—Œí„°ë‹ˆê¹Œ. ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 102.8061] 3ê¶Œ_7í™”_ì´ìƒí•œë ˆì´ë“œ Sê¸‰ í—Œí„°ì¸ í™©ë™ìˆ˜ë¥¼ ë§í•˜ëŠ” ê²ë‹ˆê¹Œ? ëŒ€ì‚¬\n",
      "[ì ìˆ˜: 109.0561] 2ê¶Œ_4í™”_ë³´ìŠ¤ì „ ëŠ¥ë ¥ì¹˜ë¥¼ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” í—Œí„°ê°€ ìˆë‹¤? ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 109.9106] 1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „ í—Œí„°í˜‘íšŒ ì†Œì† ì¤‘ ê°€ì¥ ë‚®ì€ ê³„ê¸‰, ìµœì•½ì˜ í—Œí„°. ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 110.5650] 2ê¶Œ_2í™”_ì„¸ê°€ì§€ê·œìœ¨ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦¬ê³  ìˆìœ¼ë©´ ë‹¤ë¥¸ í—Œí„°ë“¤ì´ êµ¬ì¡°í•˜ëŸ¬ ì˜¬ê¹Œìš”? ëŒ€ì‚¬\n",
      "[ì ìˆ˜: 114.7391] 1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „ ëª©ìˆ¨ì„ ê±°ëŠ” ìœ„í—˜í•œ ì§ì—… 'í—Œí„°'. ë‚˜ë¼ê³  ì¢‹ì•„ì„œ ì´ ì¼ì„ í•˜ëŠ” ê±´ ì•„ë‹ˆë‹¤. ë‚´ì ì„¤ëª…\n",
      "[ì ìˆ˜: 119.2824] 1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „ ê³ ì¡¸ ì¶œì‹ ì— ë”±íˆ ì˜í•˜ëŠ” ê²ƒë„ ì—†ëŠ” ë‚´ê²Œ, í—Œí„°ë¼ëŠ” ì§ì—…ì€ í”¼í•  ìˆ˜ ì—†ëŠ” ì„ íƒì´ì—ˆë‹¤. ë‚´ì ì„¤ëª…\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ì¸ë±ìŠ¤ì™€ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "index = faiss.read_index(\"solo_leveling_faiss_ko.index\")\n",
    "with open(\"solo_leveling_texts.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "\n",
    "# 2. ê²€ìƒ‰ì–´ ì…ë ¥\n",
    "query = \"ì„±ì§„ìš°ëŠ” ëª‡ ê¸‰ í—Œí„°?\"\n",
    "\n",
    "# 3. ì„ë² ë”© ìƒì„± ë° ê²€ìƒ‰\n",
    "query_vec = model.encode([query])\n",
    "D, I = index.search(query_vec, k=10)  # ìƒìœ„ 5ê°œ\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ ìœ ì‚¬ ì¥ë©´ Top 10 ğŸ”¹\\n\")\n",
    "for idx, score in zip(I[0], D[0]):\n",
    "    print(f\"[ì ìˆ˜: {score:.4f}] {texts[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a758d87-767b-43d6-b9fe-957b405d6f67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. ë²¡í„° DB ë¡œë“œ\u001b[39;00m\n\u001b[1;32m      6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjhgan/ko-sroberta-multitask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolo_leveling_faiss_ko\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dangerous_deserialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ğŸ”¹ ì´ ì˜µì…˜ ì¶”ê°€\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2. ì˜ˆì‹œ ì§ˆì˜\u001b[39;00m\n\u001b[1;32m     11\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì„±ì§„ìš°ëŠ” í™©ë™ì„ê³¼ ì²˜ìŒ ë§Œë‚œ ì¥ë©´ì—ì„œ ë¬´ì—‡ì„ í–ˆì–´?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:1209\u001b[0m, in \u001b[0;36mFAISS.load_local\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# load docstore and index_to_docstore_id\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 1209\u001b[0m     (\n\u001b[1;32m   1210\u001b[0m         docstore,\n\u001b[1;32m   1211\u001b[0m         index_to_docstore_id,\n\u001b[1;32m   1212\u001b[0m     ) \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(  \u001b[38;5;66;03m# ignore[pickle]: explicit-opt-in\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m         f\n\u001b[1;32m   1214\u001b[0m     )\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(embeddings, index, docstore, index_to_docstore_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle\n",
    "\n",
    "# 1. ë²¡í„° DB ë¡œë“œ\n",
    "embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')\n",
    "db = FAISS.load_local(\"solo_leveling_faiss_ko\", embedding, allow_dangerous_deserialization=True)  # ğŸ”¹ ì´ ì˜µì…˜ ì¶”ê°€\n",
    "\n",
    "\n",
    "# 2. ì˜ˆì‹œ ì§ˆì˜\n",
    "query = \"ì„±ì§„ìš°ëŠ” í™©ë™ì„ê³¼ ì²˜ìŒ ë§Œë‚œ ì¥ë©´ì—ì„œ ë¬´ì—‡ì„ í–ˆì–´?\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"[{i}] {doc.page_content}\")\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle\n",
    "\n",
    "# 1. ë²¡í„° DB ë¡œë“œ\n",
    "embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')\n",
    "db = FAISS.load_local(\"solo_leveling_faiss_ko\", embedding)\n",
    "\n",
    "# 2. ì˜ˆì‹œ ì§ˆì˜\n",
    "query = \"ì„±ì§„ìš°ëŠ” í™©ë™ì„ê³¼ ì²˜ìŒ ë§Œë‚œ ì¥ë©´ì—ì„œ ë¬´ì—‡ì„ í–ˆì–´?\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"[{i}] {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac05aed2-9dae-4f2f-b3a1-eee03a1f5c81",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open solo_leveling_faiss_ko.index for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1084854/3007134425.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mindex_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"solo_leveling_faiss_ko.index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtexts_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"solo_leveling_texts.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/faiss/swigfaiss_avx512.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11640\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open solo_leveling_faiss_ko.index for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\n",
    "index_path = \"solo_leveling_faiss_ko.index\"\n",
    "texts_path = \"solo_leveling_texts.pkl\"\n",
    "\n",
    "index = faiss.read_index(index_path)\n",
    "with open(texts_path, \"rb\") as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "# 2. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„\n",
    "embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')\n",
    "\n",
    "# 3. LangChain ë¬¸ì„œí™”\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embedding,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "# 4. LangChain í˜¸í™˜ êµ¬ì¡°ë¡œ ì €ì¥\n",
    "db.save_local(\"solo_leveling_faiss_langchain\")\n",
    "print(\"ë³€í™˜ ì™„ë£Œ: solo_leveling_faiss_langchain í´ë” ìƒì„±ë¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27fe9f-c69a-4dab-8d9f-603c7079cad6",
   "metadata": {},
   "source": [
    "## 1. tsv full data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60146aa5-f97a-4931-a4f2-7f8d33136f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ì—í”¼ì†Œë“œ                     scene_text  type\n",
      "0  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „    ë„¤, ê¹€ìƒì‹ ì•„ì €ì”¨. ì‹ ê²½ ì¨ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.    ëŒ€ì‚¬\n",
      "1  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „               ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°  ë‚´ì ì„¤ëª…\n",
      "2  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „         ë­˜ìš” í•˜í•˜... ì˜¤ëŠ˜ë„ ì˜ ë¶€íƒë“œë¦´ê²Œìš”.    ëŒ€ì‚¬\n",
      "3  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „    í—Œí„°í˜‘íšŒ ì†Œì† ì¤‘ ê°€ì¥ ë‚®ì€ ê³„ê¸‰, ìµœì•½ì˜ í—Œí„°.  ë‚´ì ì„¤ëª…\n",
      "4  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „  ì–´? ì•ˆë…•í•˜ì„¸ìš”. ì£¼í¬ ì”¨ë„ ì´ë²ˆ ë ˆì´ë“œ ê°€ì‹œëŠ”êµ°ìš”.    ëŒ€ì‚¬\n",
      "ì „ì²´ ë¬¸ì¥ ìˆ˜: 549\n",
      "ì»¬ëŸ¼ ëª©ë¡: ['ì—í”¼ì†Œë“œ', 'scene_text', 'type']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"sl_webtoon_full_data_sequential.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(\"ì „ì²´ ë¬¸ì¥ ìˆ˜:\", len(df))\n",
    "print(\"ì»¬ëŸ¼ ëª©ë¡:\", df.columns.tolist())\n",
    "\n",
    "# 549\n",
    "#ì—í”¼ì†Œë“œ, scene_text, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd35b473-3d92-4d9d-a8ee-5565dff05e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ì—í”¼ì†Œë“œ                   scene_text  type\n",
      "0  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „  ë„¤, ê¹€ìƒì‹ ì•„ì €ì”¨. ì‹ ê²½ ì¨ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.    ëŒ€ì‚¬\n",
      "1  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „             ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°  ë‚´ì ì„¤ëª…\n",
      "2  1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „       ë­˜ìš” í•˜í•˜... ì˜¤ëŠ˜ë„ ì˜ ë¶€íƒë“œë¦´ê²Œìš”.    ëŒ€ì‚¬\n",
      "ì»¬ëŸ¼: ['ì—í”¼ì†Œë“œ', 'scene_text', 'type'] ì „ì²´ í–‰: 549\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sl_webtoon_full_data_sequential.tsv\", sep=\"\\t\")\n",
    "print(df.head(3))\n",
    "print(\"ì»¬ëŸ¼:\", df.columns.tolist(), \"ì „ì²´ í–‰:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa5db259-991a-48b1-859f-2308432737c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #0 ëŒ€ì‚¬ ë„¤, ê¹€ìƒì‹ ì•„ì €ì”¨. ì‹ ê²½ ì¨ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.', '[1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #1 ë‚´ì ì„¤ëª… ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°', '[1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #2 ëŒ€ì‚¬ ë­˜ìš” í•˜í•˜... ì˜¤ëŠ˜ë„ ì˜ ë¶€íƒë“œë¦´ê²Œìš”.']\n"
     ]
    }
   ],
   "source": [
    "# â‘  ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì¶”ê°€ (ì›ë³¸ ì¶”ì ìš©)\n",
    "df['row_id'] = df.index\n",
    "\n",
    "# â‘¡ ìµœì¢… RAG ë¬¸ì¥ ì»¬ëŸ¼ ìƒì„±\n",
    "df['text'] = df.apply(\n",
    "    lambda x: f\"[{x['ì—í”¼ì†Œë“œ']}] #{x['row_id']} {x['type']} {x['scene_text']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# â‘¢ í™•ì¸\n",
    "print(df['text'].head(3).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b95c977-5485-4fdf-b5d8-fb837a0a8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ë¬¸ì¥ ìˆ˜: 549\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].tolist()\n",
    "print(\"ìµœì¢… ë¬¸ì¥ ìˆ˜:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49cc9b2e-d035-433a-9b65-94278a783e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆì‹œ 5ê°œ:\n",
      "- [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #0 ëŒ€ì‚¬ ë„¤, ê¹€ìƒì‹ ì•„ì €ì”¨. ì‹ ê²½ ì¨ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.\n",
      "- [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #1 ë‚´ì ì„¤ëª… ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°\n",
      "- [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #2 ëŒ€ì‚¬ ë­˜ìš” í•˜í•˜... ì˜¤ëŠ˜ë„ ì˜ ë¶€íƒë“œë¦´ê²Œìš”.\n",
      "- [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #3 ë‚´ì ì„¤ëª… í—Œí„°í˜‘íšŒ ì†Œì† ì¤‘ ê°€ì¥ ë‚®ì€ ê³„ê¸‰, ìµœì•½ì˜ í—Œí„°.\n",
      "- [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #4 ëŒ€ì‚¬ ì–´? ì•ˆë…•í•˜ì„¸ìš”. ì£¼í¬ ì”¨ë„ ì´ë²ˆ ë ˆì´ë“œ ê°€ì‹œëŠ”êµ°ìš”.\n",
      "\n",
      "ìµœì¢… ë¬¸ì¥ ìˆ˜: 549\n"
     ]
    }
   ],
   "source": [
    "# 2ë‹¨ê³„: ìµœì¢… RAG ë¬¸ì¥ ìƒì„±\n",
    "df['row_id'] = df.index  # ì›ë³¸ ì¶”ì ìš© ì¸ë±ìŠ¤\n",
    "df['text'] = df.apply(\n",
    "    lambda x: f\"[{x['ì—í”¼ì†Œë“œ']}] #{x['row_id']} {x['type']} {x['scene_text']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# í™•ì¸ìš© ì¶œë ¥\n",
    "print(\"ì˜ˆì‹œ 5ê°œ:\")\n",
    "for t in df['text'].head(5).tolist():\n",
    "    print(\"-\", t)\n",
    "\n",
    "# 3ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "texts = df['text'].tolist()\n",
    "print(\"\\nìµœì¢… ë¬¸ì¥ ìˆ˜:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ef1ac89-0931-48a8-9024-26150004b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²¡í„°DB ìƒì„± ì™„ë£Œ. ì´ ë¬¸ì¥ ìˆ˜: 549\n",
      "ğŸ’¾ 'solo_leveling_faiss_ko' í´ë”ì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. í•œêµ­ì–´ ë©€í‹°íƒœìŠ¤í¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')\n",
    "\n",
    "# 2. ë²¡í„°DB ìƒì„±\n",
    "db = FAISS.from_texts(texts, embedding_model)\n",
    "print(\"âœ… ë²¡í„°DB ìƒì„± ì™„ë£Œ. ì´ ë¬¸ì¥ ìˆ˜:\", len(texts))\n",
    "\n",
    "# 3. ë¡œì»¬ì— ì €ì¥\n",
    "db.save_local(\"solo_leveling_faiss_ko\")\n",
    "print(\"ğŸ’¾ 'solo_leveling_faiss_ko' í´ë”ì— ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6acad70-ae02-4808-800e-fee4c2a36153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #1 ë‚´ì ì„¤ëª… ë‚´ ì´ë¦„ì€ ì„±ì§„ìš°. Eê¸‰ í—Œí„°\n",
      "[2] [2ê¶Œ_4í™”_ë³´ìŠ¤ì „] #451 ëŒ€ì‚¬ í—Œí„° ì„±ì§„ìš°ì…ë‹ˆë‹¤.\n",
      "[3] [1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #9 ë‚´ì ì„¤ëª… Eê¸‰ í—Œí„° ì„±ì§„ìš°.\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ DB ë¡œë“œ\n",
    "db = FAISS.load_local(\"solo_leveling_faiss_ko\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ì˜ˆì‹œ ì§ˆì˜\n",
    "query = \"ì´ì¤‘ë˜ì „ì—ì„œ ì„±ì§„ìš°ëŠ” ë­í–ˆì§€?\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"[{i}] {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef2966e-c110-4565-8ddf-1a1bee864934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹µë³€: ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
      "\n",
      "ë¬¸ë§¥:\n",
      "[2ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸ ] #340 ëŒ€ì‚¬ í•˜ì§€ë§Œ ì´ ë˜ì „... ëŒ€ëµ Eê¸‰ ë˜ì „ ì•„ë‹ê¹Œ? ê·¸ë ‡ë‹¤ë©´ ê·¸ë‹¤ì§€ ê°’ì–´ì¹˜ê°€ ë†’ì§„ ì•Šê² ë‹¤.\n",
      "\n",
      "[2ê¶Œ_4í™”_ë³´ìŠ¤ì „] #451 ëŒ€ì‚¬ í—Œí„° ì„±ì§„ìš°ì…ë‹ˆë‹¤.\n",
      "\n",
      "[1ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸] #170 ëŒ€ì‚¬ Eê¸‰ ë§ˆìˆ˜í•œí…Œë„ ì©”ì©”ë§¤ë˜ ê²Œ ì—Šê·¸ì œì¸ë°, í˜¼ìì„œ ë˜ì „ì„ í´ë¦¬ì–´í•˜ë ¤ê³ ?\n",
      "\n",
      "[2ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸ ] #307 ë‚´ì ì„¤ëª… ì´ì¤‘ ë˜ì „ì—ì„œ ê¹€ìƒì‹ ì•„ì €ì”¨ê°€ ë†“ê³  ê°„ ì´ ê²€!\n",
      "\n",
      "[1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #84 ëŒ€ì‚¬ ì•„ì €ì”¨... ì´ ë˜ì „ì—” ë£°ì´ ìˆì–´ìš”.\n",
      "\n",
      "ì§ˆë¬¸:\n",
      "ì´ì¤‘ ë˜ì „ì—ì„œ ì„±ì§„ìš°ê°€ í´ë¦¬ì–´ë¥¼ ì‹œë„í–ˆë‹¤ëŠ” ê²Œ ë¬´ìŠ¨ ì˜ë¯¸ì§€?\n",
      "\n",
      "ë‹µë³€: ì´ì¤‘ ë˜ì „ì—ì„œ ì„±ì§„ìš°ê°€ í´ë¦¬ì–´ë¥¼ ì‹œë„í–ˆë‹¤ëŠ” ê²ƒì€ ê·¸ê°€ ì´ì „ì— ë°°ì› ë˜ ë˜ì „ì˜ ê·œì¹™ì´ë‚˜ ë°©ë²•ì„ ë– ì˜¬ë¦¬ë©° ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ë ¤ í–ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ì„±ì§„ìš°ê°€ ë˜ì „ í´ë¦¬ì–´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë˜í•œ, ì´ëŠ” ê·¸ê°€ ë‹¨ìˆœíˆ Eê¸‰ ë˜ì „ì„ í´ë¦¬ì–´í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ì„±ì¥í–ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.  (ì„±ì§„ìš°ê°€ ë˜ì „ í´ë¦¬ì–´ë¥¼ ì‹œë„í–ˆë‹¤ëŠ” ê²ƒì€ ê·¸ê°€ ë†’ì€ ìˆ˜ì¤€ì˜ ë‚œì´ë„ë¥¼ ê·¹ë³µí•  ì¤€ë¹„ê°€ ë˜ì–´ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ” ê·¸ì˜ ì„±ì¥ê³¼ ëŠ¥ë ¥ í–¥ìƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.)  ë”°ë¼ì„œ, ì„±ì§„ìš°ê°€ Eê¸‰ ë˜ì „ì„ í´ë¦¬ì–´í•˜ë ¤ëŠ” ì‹œë„ëŠ” ê·¸ì˜ ì„±ì¥ê³¼ ë„ì „ ì •ì‹ ì„ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ ì¥ë©´ì…ë‹ˆë‹¤.   (ì„±ì§„ìš°ê°€ Eê¸‰ ë˜ì „ì„ í´ë¦¬ì–´í•˜ë ¤ê³  ì‹œë„í•œ ê²ƒì€ ê·¸ì˜ ëŠ¥ë ¥ê³¼ ìì‹ ê°ì´ í–¥ìƒë˜ì—ˆìŒì„ ë‚˜íƒ€ë‚´ë©°, ì´ëŠ” ì´ì•¼ê¸°ì— ì¤‘ìš”í•œ ì „í™˜ì ì´ ë©ë‹ˆë‹¤.)  (ì´ ì¥ë©´ì€ ì„±ì§„ìš°ê°€ ë‹¨ìˆœí•œ ê¸°ì´ˆì ì¸ ë˜ì „ í´ë¦¬ì–´\n",
      "\n",
      "ì°¸ì¡° ë¬¸ì„œ:\n",
      "[2ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸ ] #340 ëŒ€ì‚¬ í•˜ì§€ë§Œ ì´ ë˜ì „... ëŒ€ëµ Eê¸‰ ë˜ì „ ì•„ë‹ê¹Œ? ê·¸ë ‡ë‹¤ë©´ ê·¸ë‹¤ì§€ ê°’ì–´ì¹˜ê°€ ë†’ì§„ ì•Šê² ë‹¤.\n",
      "[2ê¶Œ_4í™”_ë³´ìŠ¤ì „] #451 ëŒ€ì‚¬ í—Œí„° ì„±ì§„ìš°ì…ë‹ˆë‹¤.\n",
      "[1ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸] #170 ëŒ€ì‚¬ Eê¸‰ ë§ˆìˆ˜í•œí…Œë„ ì©”ì©”ë§¤ë˜ ê²Œ ì—Šê·¸ì œì¸ë°, í˜¼ìì„œ ë˜ì „ì„ í´ë¦¬ì–´í•˜ë ¤ê³ ?\n",
      "[2ê¶Œ_3í™”_í€˜ìŠ¤íŠ¸ ] #307 ë‚´ì ì„¤ëª… ì´ì¤‘ ë˜ì „ì—ì„œ ê¹€ìƒì‹ ì•„ì €ì”¨ê°€ ë†“ê³  ê°„ ì´ ê²€!\n",
      "[1ê¶Œ_1í™”_ì´ì¤‘ë˜ì „] #84 ëŒ€ì‚¬ ì•„ì €ì”¨... ì´ ë˜ì „ì—” ë£°ì´ ìˆì–´ìš”.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')\n",
    "vectorstore = FAISS.load_local(\"solo_leveling_faiss_ko\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\në¬¸ë§¥:\\n{context}\\n\\nì§ˆë¬¸:\\n{question}\\n\\në‹µë³€:\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": custom_prompt  }\n",
    ")\n",
    "\n",
    "\n",
    "#ì˜ˆì‹œ ì§ˆë¬¸\n",
    "query = \"ì´ì¤‘ ë˜ì „ì—ì„œ ì„±ì§„ìš°ê°€ í´ë¦¬ì–´ë¥¼ ì‹œë„í–ˆë‹¤ëŠ” ê²Œ ë¬´ìŠ¨ ì˜ë¯¸ì§€?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"ë‹µë³€:\", result[\"result\"])\n",
    "print(\"\\nì°¸ì¡° ë¬¸ì„œ:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898571b8-a683-4081-9815-76aa5b82e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1089716/3543207748.py:9: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í™©ë™ì„ í¸ ì‹œë®¬ë ˆì´ì…˜ ===\n",
      "1. 1-1. í™©ë™ì„ ë¬´ë¦¬ë¥¼ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.\n",
      "2. 1-2. í™©ë™ì„ ë¬´ë¦¬ì™€ ì§„í˜¸ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.\n",
      "3. 2. ì ì„ ë¬´ë ¥í™”í•˜ê±°ë‚˜ ê¸°ì ˆì‹œí‚¤ê³  ì‚´ë ¤ë‘”ë‹¤.\n",
      "4. 3-1. ë§ˆì •ì„ì„ ë“¤ê³  ë„ë§ì¹œë‹¤.\n",
      "5. 3-2. ì‹œìŠ¤í…œì„ ê±°ë¶€í•˜ê³  ê·¸ëƒ¥ ë„ë§ì¹œë‹¤.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ì„ íƒ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:  2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, choice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(choices, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchoice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m user_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mì„ íƒ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m selected \u001b[38;5;241m=\u001b[39m choices[user_choice\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[ì„ íƒ]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '2.'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# ======================\n",
    "# 1. TSV ë°ì´í„° ë¡œë“œ\n",
    "# ======================\n",
    "file_path = \"Sl_lizard.tsv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# ======================\n",
    "# 2. ì„ íƒì§€ ì •ì˜\n",
    "# ======================\n",
    "choices = [\n",
    "    \"1-1. í™©ë™ì„ ë¬´ë¦¬ë¥¼ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.\",\n",
    "    \"1-2. í™©ë™ì„ ë¬´ë¦¬ì™€ ì§„í˜¸ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë‘ ì²˜ì¹˜í•œë‹¤.\",\n",
    "    \"2. ì ì„ ë¬´ë ¥í™”í•˜ê±°ë‚˜ ê¸°ì ˆì‹œí‚¤ê³  ì‚´ë ¤ë‘”ë‹¤.\",\n",
    "    \"3-1. ë§ˆì •ì„ì„ ë“¤ê³  ë„ë§ì¹œë‹¤.\",\n",
    "    \"3-2. ì‹œìŠ¤í…œì„ ê±°ë¶€í•˜ê³  ê·¸ëƒ¥ ë„ë§ì¹œë‹¤.\"\n",
    "]\n",
    "\n",
    "print(\"=== í™©ë™ì„ í¸ ì‹œë®¬ë ˆì´ì…˜ ===\")\n",
    "for idx, choice in enumerate(choices, start=1):\n",
    "    print(f\"{idx}. {choice}\")\n",
    "\n",
    "user_choice = int(input(\"ì„ íƒ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \"))\n",
    "selected = choices[user_choice-1]\n",
    "print(f\"\\n[ì„ íƒ]: {selected}\")\n",
    "\n",
    "# ======================\n",
    "# 3. ê·¼ê±° ê²€ìƒ‰ (ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜)\n",
    "# ======================\n",
    "# ì„ íƒì§€ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì˜ˆ: ë„ë§, ì²˜ì¹˜, ê¸°ì ˆ)\n",
    "keywords = [\"ì²˜ì¹˜\", \"ì‚´ë ¤\", \"ë„ë§\", \"ê±°ë¶€\"]\n",
    "matched_kw = None\n",
    "for kw in keywords:\n",
    "    if kw in selected:\n",
    "        matched_kw = kw\n",
    "        break\n",
    "\n",
    "if matched_kw:\n",
    "    related = df[df['ìƒí™©'].str.contains(matched_kw, na=False) | df['ëŒ€ì‚¬'].str.contains(matched_kw, na=False)]\n",
    "else:\n",
    "    related = df\n",
    "\n",
    "print(\"\\n--- ê´€ë ¨ ê·¼ê±° ìƒí™© ---\")\n",
    "print(related[['ìƒí™©', 'ëŒ€ì‚¬']].head(5))\n",
    "\n",
    "# ======================\n",
    "# 4. ì¹´ë‚˜ë‚˜ ëª¨ë¸ ì—°ê²°\n",
    "# ======================\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"kakaocorp/kanana-nano-2.1b-instruct\",\n",
    "    device=0,  # GPU ì‚¬ìš©\n",
    ")\n",
    "\n",
    "context_text = \"\\n\".join(\n",
    "    (related['ìƒí™©'].head(3).tolist() + related['ëŒ€ì‚¬'].head(3).tolist())\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "ìƒí™©: {context_text}\n",
    "ì‚¬ìš©ì ì„ íƒ: {selected}\n",
    "\n",
    "ì„±ì§„ìš°ì˜ ë§íˆ¬ë¡œ, í•´ë‹¹ ìƒí™©ì—ì„œ í•œ ì¤„ ëŒ€ì‚¬ë¥¼ ìƒì„±í•˜ë¼.\n",
    "\"\"\"\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(\"\\n[ì„±ì§„ìš° ëŒ€ì‚¬ ìƒì„±]\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46946824-27c5-4a95-a293-d6b3ab905277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ka)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
